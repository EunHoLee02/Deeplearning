{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f02bdfec278db2",
   "metadata": {},
   "source": [
    "a_tensor_initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40aad4b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T12:17:29.761499Z",
     "start_time": "2025-09-15T12:16:57.877557Z"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab96c6ae24bcace7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T12:45:31.567987Z",
     "start_time": "2025-09-15T12:45:31.107516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1,2,3], device='cpu')\n",
    "print(t1.dtype)     # >>> torch.float32 # >>> Tensor는 float32임\n",
    "print(t1.device)    # >>> cpu\n",
    "print(t1.requires_grad) # >>> Fasle  해당 텐서에 대한 연산 과정을 추적하고, 자동으로 미분을 계산할지 여부를 결정하는 속성이다. 기본값은 False이다.requires_grad=True인 텐서는 이후 계산을 그래프로 기록해서 backward()를 호출했을때 자동을 미분 결과를 구해준다.\n",
    "print(t1.size())    # torch.Size([3])\n",
    "print(t1.shape)     # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda() #텐서를 gpu메모리로 옮김\n",
    "t1_cpu = t1.cpu()   # 텐서를 cpu메모리로 옮기는 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a32a4007ceb3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T12:50:34.936932Z",
     "start_time": "2025-09-15T12:50:34.901654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1,2,3], device='cpu')\n",
    "print(t2.dtype) # >>> torch.int64 # >>> tensor는 int64임\n",
    "print(t2.device) # >>> cpu\n",
    "print(t2.requires_grad) # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58a1b657b15007d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T11:13:53.267384Z",
     "start_time": "2025-09-16T11:13:53.160717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 65\u001b[0m\n\u001b[0;32m     57\u001b[0m a10 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([            \u001b[38;5;66;03m#shape: torch.Size([4,1,5]), ndims(=rank): 3\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     59\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     60\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     61\u001b[0m     [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]],\n\u001b[0;32m     62\u001b[0m ])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(a10\u001b[38;5;241m.\u001b[39mshape, a10\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m---> 65\u001b[0m a11 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([            \u001b[38;5;66;03m#ValueError: expected sequence of length 3 at dim 3 (got 2)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],      \u001b[38;5;66;03m#에러 이유: PyTorch에서 torch.tensor로 텐서를 생성할때는 모든 차원(dim)의 길이가 같아야 하는데 길이가 3,2이기 때문에 에러가 난다.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m     68\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m     69\u001b[0m     [[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]]],\n\u001b[0;32m     70\u001b[0m ])\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a1 = torch.tensor(1)            #shape:torch.Size([]), ndims(=rank):0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])          #shape: torch.Size([1]), ndims(=rank):1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch. tensor([1,2,3,4,5]) #shape: torch.Size([5]), ndims(=rank):1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1],[2],[3],[4],[5]])    #shape: torch.Size([5,1]), ndims(=rank) :2\n",
    "\n",
    "a5 = torch.tensor([             #shape: torch.Size([3,2]), ndims(=rank):2\n",
    "    [1,2],\n",
    "    [3,4],\n",
    "    [5,6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([             #shape: torch.Size([3,1,2,1]),ndims(=rank):3\n",
    "    [[1],[2]],\n",
    "    [[3],[4]],\n",
    "    [[5],[6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([             #shape: torch.Size([3,1,2,1)], ndims(=rank): 4\n",
    "    [[[1],[2]]],\n",
    "    [[[3],[4]]],\n",
    "    [[[5],[6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([             #shape: torch.Size([3,1,2,3]), ndims(=rank):4\n",
    "    [[[1,2,3],[2,3,4]]],\n",
    "    [[[3,1,1],[4,4,5]]],\n",
    "    [[[5,6,2],[6,3,1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "a9 = torch.tensor([             #shape: torch.Size([3,1,2,3,1]), ndims(=rank) : 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([            #shape: torch.Size([4,5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([            #shape: torch.Size([4,1,5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([            #ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],      #에러 이유: PyTorch에서 torch.tensor로 텐서를 생성할때는 모든 차원(dim)의 길이가 같아야 하는데 길이가 3,2이기 때문에 에러가 난다.\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92747d2e16430b1",
   "metadata": {},
   "source": [
    "b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb60449e749cc2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T11:28:49.675090Z",
     "start_time": "2025-09-16T11:28:49.663853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "l1 = [1,2,3]\n",
    "t1 = torch.Tensor(l1)           #클래스 생성자, float32, 복사\n",
    "\n",
    "l2 = [1,2,3]\n",
    "t2 = torch.tensor(l2)           #팩토리 함수, dtype 자동 추론(int64), 복사\n",
    "\n",
    "l3 = [1,2,3]\n",
    "t3 = torch.as_tensor(l3)        # tensor는 원본을 복사해서 새 텐서를 만들지만 as_tensor는 원본과 연결되어 복사를 최소화 한다. tensor는 원본의 값이 변경되어도 영향을 받지 않지만, as_tensor는 원본이 변경되면 같이 변경되어 예기치 않은 버그를 일으킬수있다.하지만 여기서 l3는 파이썬 리스트이므로 원본과 공유되지 않는다. numpy를 사용해 np.array로 만든 배열인 경우에만 공유가 된다.\n",
    "\n",
    "l1[0] = 100                     # 원본 리스트 변경\n",
    "l2[0] = 100                     # 원본 리스트 변경\n",
    "l3[0] = 100                     # 원본 리스트 변경\n",
    "\n",
    "print(t1)                       # Tensor는 float32이기 때문에 tensor([1,2,3])이 아닌 tensor([1., 2., 3.])처럼 부동소수점으로 출력된다.\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1219f9279e2be95a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T11:40:54.768929Z",
     "start_time": "2025-09-16T11:40:54.731057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l4 = np.array([1,2,3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 =np.array([1,2,3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1,2,3])\n",
    "t6 = torch.as_tensor(l6)        #np.array이므로 원본이 변경되면 텐서도 같이 변경\n",
    "\n",
    "l4[0] = 100                     # 원본 리스트 변경\n",
    "l5[0] = 100                     # 원본 리스트 변경\n",
    "l6[0] = 100                     # 원본 리스트 변경\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cd34619f7a035",
   "metadata": {},
   "source": [
    "c_tensor_initialization_constant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b494f2bc533c4207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T11:57:34.511129Z",
     "start_time": "2025-09-16T11:57:34.411376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 2., 3., 0.])\n",
      "tensor([-3.0769e-05,  1.0258e-42,  0.0000e+00,  0.0000e+00])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "                                #*_like(tensor) -> 주어진 텐서와 같은 shape과 dtype으로 새 텐서를 생성\n",
    "                                # (n,)\n",
    "                                # 파이썬 에서는 길이가 1인 튜플\n",
    "                                # PyTorch에서 size 인자는 텐서의 shape을 나타내는 튜플이어야 한다.\n",
    "                                # 그래서 길이가 n인 1차원 텐서이다.\n",
    "t1 = torch.ones(size=(5,))      # or torch.ones(5)\n",
    "                                # 길이 5짜리 1차원 텐서를 모두 1로 채움\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)                       # >>> tensor([1.,1.,1.,1.,1.])\n",
    "print(t1_like)                  # >>> tensor([1.,1.,1.,1.,1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))     # or torch.zeros(6)\n",
    "                                # 길이 6짜리 1차원 텐서를 모두 0으로 채움\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)                       # >>> tensor([0.,0.,0.,0.,0.,0.])\n",
    "print(t2_like)                  # >>> tensor([0.,0.,0.,0.,0.,0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))     # or torch.zeros(4)\n",
    "                                # 초기화를 하지 않고 길이 4짜리 1차원 텐서를 모두 비어있는 값으로 채움\n",
    "                                # 실행환경에 따라서 쓰레기 값이 나올수있음\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)                       # >>> tensor([0.,0.,0.,0.])\n",
    "print(t3_like)                  # >>> tensor([0.,0.,0.,0.])\n",
    "\n",
    "t4 = torch. eye(n=3)            # eye(n) -> nxn 단위행렬 생성\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6daed3ddfa7916c",
   "metadata": {},
   "source": [
    "d_tensor_initialization_random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c226314ef77d2ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T12:35:03.004755Z",
     "start_time": "2025-09-16T12:35:02.907479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 15]])\n",
      "tensor([[0.0578, 0.3304, 0.5857]])\n",
      "tensor([[0.4727, 0.3563, 0.7999]])\n",
      "tensor([[ 8.6892, 10.5474],\n",
      "        [ 7.6586, 11.3469],\n",
      "        [ 8.4642,  9.9671]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1,2))     # shape이 [1,2] 인 텐서를 10이상 20미만의 정수 랜덤값 생성하여 채움\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1,3))                         # shape이 [1,3]인 텐서를 [0,1) 구간에서 균등 분포를 따르는 난수로 채움\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1,3))                        # shape이 [1,3]인 텐서를 평균이 0이고 표준편차가 1인 정규분포 난수로 채움\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3,2))   # shape이 [3,2]인 텐서를 평균 10.0, 표준편차 1.0인 정규분포에서 난수 생성후 채움\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)    # [0.0, 5.0] 구간을 3등분해서 출력\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)                                # 0부터 5미만까지 1씩 증가\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ac6e9e1b94e566",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T12:38:13.219781Z",
     "start_time": "2025-09-16T12:38:13.047726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# seed를 고정하면 난수가 재현 가능해진다.\n",
    "# random1, random2는 서로 다른 난수지만, 항상 같은 시드라면 실행할 때마다\n",
    "# 동일한 값이 나온다.\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2,3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2,3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "# 시드를 다시 1729로 고정했으므로\n",
    "# random3 == random1\n",
    "# random4 == random2이다.\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2,3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2,3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea367ccba6c8e82",
   "metadata": {},
   "source": [
    "e_tensor_type_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c202153637e319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T01:00:43.066200Z",
     "start_time": "2025-09-17T01:00:43.052111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2,3))               # 2*3 크기의 텐서를 1로 초기화\n",
    "print(a.dtype)                      # >>> tortch\n",
    "\n",
    "b = torch.ones((2,3), dtype=torch.int16)    # 정수형 16비트 텐서 생성\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2,3), dtype=torch.float64)*20.  # torch.rand [0,1) 구간 균등분포 랜덤값\n",
    "                                                # 실수형 64비트 텐서 생성\n",
    "                                                # *20. -> 값의 범위를 0~20 으로 스케일링\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32)                           # dtype int16에서 int32로 변환\n",
    "print(d)\n",
    "\n",
    "double_d = torch.zeros(10,2).double()\n",
    "short_e = torch.tensor([[1,2]],dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10,2).double()\n",
    "short_e = torch.ones(10,2).short()\n",
    "\n",
    "double_d =torch.zeros(10,2).type(torch.double)\n",
    "short_e = torch.ones(10,2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10,2).type(torch.double)\n",
    "short_e = torch.ones(10,2).type(dtype=torch.short)\n",
    "\n",
    "# 위의 방법 모두 dytpe 형변환 방법임 결과는 같음\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "# PyTorch는 연산시 더 정밀한 dtype으로 자동 승격\n",
    "double_f = torch.rand(5, dtype=torch.double) # dtype == float64\n",
    "short_g = double_f.to(torch.short)           # dtype == int 16\n",
    "print((double_f * short_g).dtype)           # >>> torch.float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8cd0d44566611",
   "metadata": {},
   "source": [
    "f_tensor_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b774c52c069bb0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T01:30:03.462380Z",
     "start_time": "2025-09-17T01:30:03.393674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2,3))\n",
    "t2 = torch.ones(size=(2,3))\n",
    "t3 = torch.add(t1,t2)           # t1 + t2와 동일\n",
    "t4 = t1 + t2                    # 연산자 오버로딩 사용\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402eee7a4ddc6dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T01:31:20.420121Z",
     "start_time": "2025-09-17T01:31:20.409176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.sub(t1,t2)           # t1 - t2와 동일\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34cef192b184d10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T01:31:59.485568Z",
     "start_time": "2025-09-17T01:31:59.468650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.mul(t1,t2)           # t1 * t2와 동일\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "266a97b2f6f0c267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T01:32:41.148929Z",
     "start_time": "2025-09-17T01:32:41.129946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.div(t1,t2)           # t1 / t2와 동일\n",
    "t10 = t1/t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8739e360d2f820",
   "metadata": {},
   "source": [
    "g_tensor_operations_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942aab2aad93b08f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T04:50:33.941803Z",
     "start_time": "2025-09-17T04:50:33.922073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(             # torch.dot은 1차원 벡터 두개의 내적을 계산\n",
    "    torch.tensor([2,3]), torch.tensor([2,1])\n",
    ")                           # 결과 : tensor(7)로 스칼라 값임\n",
    "print(t1,t1.size())         # 스칼라값이므로 0차원 텐서임\n",
    "\n",
    "t2 = torch.randn(2,3)       # (2x3) 행렬\n",
    "t3 = torch.randn(3,2)       # (3x2) 행렬\n",
    "t4 = torch.mm(t2,t3)        # torch.mm은 2차언 행렬 곱셉을 의미\n",
    "                            # 결과 (2x2) 행렬\n",
    "print(t4,t4.size())\n",
    "\n",
    "t5 = torch.randn(10,3,4)    # (batch=10, 3x4 행렬)\n",
    "t6 = torch.randn(10,4,5)    # (batch=10, 4x5 행렬)\n",
    "t7 = torch.bmm(t5,t6)       # (batch=10, 3x5 행렬 결과)\n",
    "                            # batch란 데이터를 한번에 묶어서 처리하는 단위\n",
    "                            # ex) 1000개의 셈플, batch size=100이면 10번의 반복이 필요\n",
    "                            # 이러한 과정을 한 epoch라고 함\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e1ce980feab8",
   "metadata": {},
   "source": [
    "h_tensor_operations_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a6ca8263d11bf10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T05:35:24.194930Z",
     "start_time": "2025-09-17T05:35:24.077182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector : dot product\n",
    "t1 = torch.randn(3)                 # shape(3,)\n",
    "t2 = torch.randn(3)                 # shape(3,)\n",
    "print(torch.matmul(t1,t2).size())   # torch.Size([])\n",
    "                                    # 1차원 벡터끼리 곱하면 스칼라값이 나와 차원이 0이다.\n",
    "\n",
    "# matrix x vector: broadcaste dot\n",
    "t3 = torch.randn(3,4)               # shape(3,4)\n",
    "t4 = torch.randn(4)                 # shape(4,)\n",
    "print(torch.matmul(t3,t4).size())   # torch.Size([3])\n",
    "                                    # (3x4)행렬 x (4,)행렬 결과는 (3,)벡터이다.\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10,3,4)            # (batch=10, 3x4 행렬들)\n",
    "t6 = torch.randn(4)                 # (4,) 벡터\n",
    "print(torch.matmul(t5,t6).size())   # torch.Size([10,3])\n",
    "                                    # (3x4)행렬 x (4,) 행렬 결과는 (3,)\n",
    "                                    # 이걸 10개 batch에 대해 반복하므로\n",
    "                                    # 결과 shape = (10,3)\n",
    "\n",
    "# bathed matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10,3,4)            # (batch=10, 3x4 행렬들)\n",
    "t8 = torch.randn(10,4,5)            # (batch=10, 4x5 행렬들)\n",
    "print(torch.matmul(t7,t8).size())   # torch.Size([10,3,5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10,3,4)            # (batch=10, 3x4 행렬들)\n",
    "t10 = torch.randn(4,5)              # (4x5) 행렬\n",
    "print(torch.matmul(t9,t10).size())  # torch.Size([10,3,5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4440bda833bc51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T05:36:15.311415Z",
     "start_time": "2025-09-17T05:36:13.457504Z"
    }
   },
   "source": [
    "i_tensor_broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329d1b2ef7906c68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:03:22.324643Z",
     "start_time": "2025-09-17T14:03:15.651554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0,2.0,3.0])\n",
    "t2 = 2.0\n",
    "print(t1*t2)\n",
    "# element-wise 연산으로 스칼라 2.0이 벡터 t1의 모든 원소에 곱해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bca1ca825f5c567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:12:38.303138Z",
     "start_time": "2025-09-17T14:12:38.296988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t3 = torch.tensor([[0,1],[2,4],[10,10]])\n",
    "t4 = torch.tensor([4,5])\n",
    "print(t3-t4)\n",
    "# t3는 (3x2), t4는 (2,) -> PyTorch가 마지막 차원을 맞춰서 브로드 캐스팅\n",
    "# 각 행을 [4,5] 뺀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca34c4e063b102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t5 = torch.tensor([[1.,2.],[3.,4.]])\n",
    "print(t5 + 2.0)     #t5.add(2.0)\n",
    "print(t5 - 2.0)     #t5.sub(2.0)\n",
    "print(t5 * 2.0)     #t5.mul(2.0)\n",
    "print(t5 / 2.0)     #t5.div(2.0)\n",
    "# 스칼라 연산도 자동 브로드캐스팅된다. 따라서 모든 원소에 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6de8dffddec5215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:18:31.078766Z",
     "start_time": "2025-09-17T14:18:31.070049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def normalize(x):\n",
    "    return x/255\n",
    "\n",
    "t6 = torch.randn(3,28,28)\n",
    "print(normalize(t6).size())\n",
    "# 픽셀값(0~255)을  0~1 범위로 맞추는 전처리 예제이다. shape 변화는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63da7384695485a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:22:28.049540Z",
     "start_time": "2025-09-17T14:22:28.029491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t7 = torch.tensor([[1,2],[0,3]])    # torch.Size([2,2])\n",
    "t8 = torch.tensor([[3,1]])          # torch.Size([1,2])\n",
    "t9 = torch.tensor([[5],[2]])        # torch.Size([2,1])\n",
    "t10 = torch.tensor([7])             # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[8, 9], [7, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d5b520b11f8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:32:40.565320Z",
     "start_time": "2025-09-17T14:32:40.554768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t11 = torch.ones(4,3,2)\n",
    "t12 = t11 * torch.rand(3,2)     # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "                                # (3,2)가 브로드캐스트 -> 결과 (4,3,2)\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4,3,2)\n",
    "t14 = t13 * torch.rand(3,1)     # 3rd dim = 1, 2nd dim is identical to t13\n",
    "                                # (3,1) -> (3,2)로 브로드캐스트 -> 결과 (4,3,2)\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4,3,2)\n",
    "t16 = t15 * torch.rand(1,2)     # 3rd dim is identical to t15, 2nd dim is 1\n",
    "                                # (1,2) -> (3,2)로 브로캐스트 -> 결과 (4,3,2)\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5,3,4,1)\n",
    "t18 = torch.rand(3,1,1)         # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "                                # (3,1,1) -> (3,4,1)로 브로드캐스트 -> 결과 (5,3,4,1)\n",
    "print((t17 + t18).size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11902eab0cc01728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T14:34:07.768188Z",
     "start_time": "2025-09-17T14:34:07.757978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "# 모든 경우에서 브로캐스팅 규칙에 따라 shape이 확장된다.\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "# 브로드캐스팅은 뒤에서부터 차원을 비교한다. 만약 크기가 서로 다르고, 둘다 1이 아닌 경우 RuntimeError 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e845dfe96bae6450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([5,5,5,5])\n",
    "# element-wise연산으로 스칼라 곱을 하면된다\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([25,25,25,25])\n",
    "# elsement-wise연산으로 각원소를 2의 거듭제곱을 한다.\n",
    "exp = torch.arange(1., 5.)  # tensor([1.,2.,3.,4.])\n",
    "a = torch.arange(1., 5.)  # tensor([1.,2.,3.,4.])\n",
    "t29 = torch.pow(a, exp)     # element-wise연산으로 각 원소끼리 거듭제곱 한다.\n",
    "print(t29)  # >>> tensor([1.,4.,27.,256.])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd89293e4b2ab8",
   "metadata": {},
   "source": [
    "j_tensor_indexing_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58a6d9942c9331d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:19:12.986823Z",
     "start_time": "2025-09-17T15:19:12.978092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])  # 1행 출력\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])    # 모든행에서 1번째 열만 뽑기\n",
    "print(x[1, 2])  # >>> tensor(7)             # 1행 2열 원소\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)    # 모든행에서 마지막 열 뽑기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5c0cafed01c397c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:18:32.584216Z",
     "start_time": "2025-09-17T15:18:32.577241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])        # 1행부터 끝까지 슬라이싱\n",
    "print(x[1:, 3:])    # >>> tensor([[ 8,  9], [13, 14]])\n",
    "                    # 1행부터 끝까지, 그리고 3열 부터 끝까지 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49a9a2aba61e396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:18:43.526363Z",
     "start_time": "2025-09-17T15:18:43.517709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))     #6x6 영행렬 생성\n",
    "y[1:4, 2] = 1               # 1~3행, 2열에 1 대입\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])          # 위에서 수정된 부분만 잘라낸 3x3 서브 매트릭스\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c626badf013facc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:09:26.184802Z",
     "start_time": "2025-09-18T02:09:26.165012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(       # z.shape = (3,4)\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])            # 앞에서 2행만 슬라이싱\n",
    "print(z[1:, 1:3])       # 1행부터 끝까지, 1~2열 슬라이싱\n",
    "print(z[:, 1:])         # 모든행, 1열부터 끝까지 슬라이싱\n",
    "\n",
    "z[1:, 1:3] = 0          # 1행부터 끝까지, 1열부터 2열 구간을 0으로 변경\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc9cbfc3a1a8a4",
   "metadata": {},
   "source": [
    "k_tensor_reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e19aa22492113d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:18:03.310348Z",
     "start_time": "2025-09-18T02:18:03.277915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "t2 = t1.view(3,2)   #shape becomes (3,2)\n",
    "                    # view: 메모리 연속성을 유지한채로 모양만 바꿈 (조건 맞지 않으면 오류 가능)\n",
    "t3 = t1.reshape(1,6)    #shape becomes (1,6)\n",
    "                        # reshape: 내부적으로 필욯면 데이터를 복사해서라도 모양을 맞춤\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "# torch.arange() 는 일정한 간격으로 숫작가 증가하는 1차원 텐서를 만드는 함수\n",
    "t4 = torch.arange(8).view(2,4)  #shape becomes (2,4)\n",
    "                                # 8개의 원소를 2행 4열로 배치\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "                                    # 8개의 원소를 2행 3열로 배치\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9f423c475beaebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:20:37.115967Z",
     "start_time": "2025-09-18T02:20:37.073585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1 # 크기가 1인 모든 차원 삭제\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0    #지정한 dim이 0인 차원 삭제\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "232d9af708f88875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:22:12.851358Z",
     "start_time": "2025-09-18T02:22:12.827185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1 # 해당위치에 새로운 차원 (크기=1)을 추가함\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e08ee8d56e0d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:24:06.894991Z",
     "start_time": "2025-09-18T02:24:06.858430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)   # 모든 차원을 하나로 펼친다.\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)   # 특정 차원(1)부터 펼친다.\n",
    "\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1b1e829b9c96ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:25:01.621948Z",
     "start_time": "2025-09-18T02:25:01.575538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])        #premute(dims) : 원하는 순서대로 차원 재배치\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)    # Shape becomes (3, 2)\n",
    "                                    # transpose(dim0,dim1): 두 차원의 위치를 맞바꿈\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "                    # 2차원 행렬의 전치 단축함수\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d0371db945051",
   "metadata": {},
   "source": [
    "l_tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cd34821577333ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:03:53.053572Z",
     "start_time": "2025-09-18T03:03:53.042746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2,1,3])\n",
    "t2 = torch.zeros([2,3,3])\n",
    "t3 = torch.zeros([2,2,3])\n",
    "\n",
    "t4 = torch.cat([t1,t2,t3], dim=1)   # dim=1 을 합친다.\n",
    "print(t4.shape) #세 텐서 모두 (2,*,3) 모양이라 두 번째 축만 늘어난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e43d41503afde59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:05:37.020988Z",
     "start_time": "2025-09-18T03:05:37.010315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0,3)  # tensor([0,1,2])\n",
    "t6 = torch.arange(3,8)  # tensor([3,4,5,6,7])\n",
    "\n",
    "t7 = torch.cat((t5,t6), dim=0) # 벡터 연결은 단순히 이어붙이는 것이다.\n",
    "print(t7.shape)     #>>> torch.Size([8])\n",
    "print(t7)       # >>> tensor([0,1,2,3,4,5,6,7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34822b568f910571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:15:08.957774Z",
     "start_time": "2025-09-18T03:15:08.943548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0,6).reshape(2,3)     #torch.Size([2,3])\n",
    "t9 = torch.arange(6,12).reshape(2,3)    #torch.Size([2,3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8,t9), dim=0 )    # (2,3) + (2,3) -> (4,3)\n",
    "print(t10.size())   # >>> torch.Size([4,3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8,t9), dim=1)     # (2,3) + (2,3) -> (2,6)\n",
    "print(t11.size())   # >>> torch.Size([2,6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de97b58104d405d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:17:19.114380Z",
     "start_time": "2025-09-18T03:17:19.100764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12,t13,t14), dim=0)   # (2,3) + (2,3) + (2,3) -> (6,3)\n",
    "print(t15.size())   #>>> torch.Size([6,3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12,t13,t14), dim=1)   # (2,3) + (2,3) + (2,3) -> (2,9)\n",
    "print(t16.size())   # >>> torch.Size([2,9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b56f62f0a072dec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:24:03.885929Z",
     "start_time": "2025-09-18T03:24:03.872442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 텐서\n",
    "t17 = torch.arange(0,6).reshape(1,2,3) #torch.Size([1,2,3])\n",
    "t18 = torch.arange(6,12).reshape(1,2,3) # torch.Size([1,2,3])\n",
    "\n",
    "t19 = torch.cat((t17,t18), dim=0)   # (1,2,3) + (1,2,3) -> (2,2,3)\n",
    "print(t19.size())   # >>> torch.Size([2,2,3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17,t18), dim=1)   # (1,2,3) + (1,2,3) -> (1,4,3)\n",
    "print(t20.size())   # >>> torch.Size([1,4,3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17,t18), dim=2)   # (1,2,3) + (1,2,3) -> (1,2,6)\n",
    "print(t21.size())   # >>> torch.Size([1,2,6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb307685a17af52",
   "metadata": {},
   "source": [
    "m_tensor_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4702cb7a33857eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:28:40.747224Z",
     "start_time": "2025-09-18T03:28:40.725804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.cat(tensor, dim)\n",
    "# 이미 존재하는 차원을 기준으로 이어붙임\n",
    "# 따라서 보통 unsqueeze로 차원을 맞춰줘야 원하는 모양이 나온다.\n",
    "\n",
    "# torch.stack(tensors,dim)\n",
    "# 새로운 차원을 하나 추가한 뒤, 그 차원에서 쌓음\n",
    "# unsqueeze를 따로 안 써도 된다.\n",
    "\n",
    "t1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "t2 = torch.tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "t3 = torch.stack([t1,t2],dim=0) # stach은 자동으로 차원 추가\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0),t2.unsqueeze(dim=0)],dim=0) #cat은 unsqeueze로 직접 차원을 맞춘 후 이어 붙임\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1,t2],dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1),t2.unsqueeze(dim=1)],dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1,t2],dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2),t2.unsqueeze(dim=2)],dim=2)\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff16063a00015442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:37:20.234561Z",
     "start_time": "2025-09-18T03:37:20.217825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0,3)  # tensor([0,1,2])\n",
    "t10 = torch.arange(3,6) # tensor([3,4,5])\n",
    "\n",
    "print(t9.size(),t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9,t10),dim=0)\n",
    "print(t11.size())   # >>>torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0),t10.unsqueeze(0)),dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9,t10),dim=1)\n",
    "print(t13.size())   # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1),t10.unsqueeze(1)),dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ecb413d964822",
   "metadata": {},
   "source": [
    "n_tensor_vstack_htack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90cc919e6e1567a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:59:46.597125Z",
     "start_time": "2025-09-18T03:59:46.569450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1,2,3])\n",
    "t2 = torch.tensor([4,5,6])\n",
    "t3 = torch.vstack((t1,t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1],[2],[3]])\n",
    "t5 = torch.tensor([[4],[5],[6]])\n",
    "t6 = torch.vstack((t4,t5))          #1차원 텐서를 행방향으로 쌓는다\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "    [[1,2,3],[4,5,6]],\n",
    "    [[7,8,9],[10,11,12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2,2,3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "    [[13,14,15],[16,17,18]],\n",
    "    [[19,20,21],[22,23,24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2,2,3)\n",
    "\n",
    "t9 = torch.vstack([t7,t8])\n",
    "print(t9.shape)\n",
    "# >>> (4,2,3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9042ac24bdb3c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1,2,3])\n",
    "t11 = torch.tensor([4,5,6])\n",
    "t12 = torch.hstack((t10,t11))   #1차원 텐서는 단순히 옆으로 이어붙인다.(dim=0 기준)\n",
    "print(t12)\n",
    "# >>> tensor([1,2,3,4,5,6])\n",
    "\n",
    "t13 = torch.tensor([[1],[2],[3]])\n",
    "t14 = torch.tensor([[4],[5],[6]])\n",
    "t15 = torch.hstack((t13,t14))   #열 방향으로 합쳐진다.\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "\n",
    "# torch.vstack : 세로방향\n",
    "# 1차원은 행으로 쌓임\n",
    "# 2차원 이상은 첫 번째 차원(batch 차원) 기준으로 결합\n",
    "\n",
    "# torch.hstack : 가로방향\n",
    "# 1차원은 단순히 옆으로 이어붙임\n",
    "# 2차원 이상은 두 번째 차원 기준으로 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541cfe4a-82b3-46ff-aabd-a3a17e7cce88",
   "metadata": {},
   "source": [
    "## 숙제후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b53b2-cd9a-4312-bb71-0d43bd63e43b",
   "metadata": {},
   "source": [
    "처음 숙제를 하기전에 그래도 tensor에 대해 공부를 하고 적어서 공부를 하는게 낫지않을까? 라는 생각이 있어서 시작하기가 막막했는데 막상 그냥 시작해보니까 글로 읽을 때보다 훨씬 이해가 쉬웠습니다. 그리고 수업시간에 수업내용을 이해하기가 진짜 어려웠는데 이 숙제를 하면서 좀더 이해하기 수월해져서 정말 도움이 많이 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adc846-8891-4854-8858-01dafd70870a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
